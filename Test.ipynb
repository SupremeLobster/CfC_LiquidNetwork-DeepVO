{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd96055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vilad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from params import par\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import kaiming_normal_, orthogonal_\n",
    "import numpy as np\n",
    "from CfC.torch_cfc import Cfc\n",
    "import random\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "CFC = {\n",
    "    \"epochs\": 100,\n",
    "    \"clipnorm\": 0,\n",
    "    \"hidden_size\": 448,\n",
    "    \"base_lr\": 0.002,\n",
    "    \"decay_lr\": 0.97,\n",
    "    \"backbone_activation\": \"silu\",\n",
    "    \"backbone_units\": 64,\n",
    "    \"backbone_layers\": 1,\n",
    "    \"backbone_dr\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"tau\": 10,\n",
    "    \"batch_size\": 8,\n",
    "    \"optim\": \"adamw\",\n",
    "    \"init\": 0.84,\n",
    "    \"use_mixed\": False,\n",
    "}\n",
    "\n",
    "def conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, dropout=0):\n",
    "    if batchNorm:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(dropout)#, inplace=True)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(dropout)#, inplace=True)\n",
    "        )\n",
    "\n",
    "class DeepVO(nn.Module):\n",
    "    def __init__(self, imsize1, imsize2, batchNorm=True):\n",
    "        super(DeepVO,self).__init__()\n",
    "        # CNN\n",
    "        self.batchNorm = batchNorm\n",
    "        self.clip = par.clip\n",
    "        self.conv1   = conv(self.batchNorm,   2,   64, kernel_size=7, stride=2, dropout=par.conv_dropout[0])\n",
    "        self.conv2   = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2, dropout=par.conv_dropout[1])\n",
    "        self.conv3   = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2, dropout=par.conv_dropout[2])\n",
    "        self.conv3_1 = conv(self.batchNorm, 256,  256, kernel_size=3, stride=1, dropout=par.conv_dropout[3])\n",
    "        self.conv4   = conv(self.batchNorm, 256,  512, kernel_size=3, stride=2, dropout=par.conv_dropout[4])\n",
    "        self.conv4_1 = conv(self.batchNorm, 512,  512, kernel_size=3, stride=1, dropout=par.conv_dropout[5])\n",
    "        self.conv5   = conv(self.batchNorm, 512,  512, kernel_size=3, stride=2, dropout=par.conv_dropout[6])\n",
    "        self.conv5_1 = conv(self.batchNorm, 512,  512, kernel_size=3, stride=1, dropout=par.conv_dropout[7])\n",
    "        self.conv6   = conv(self.batchNorm, 512, 1024, kernel_size=3, stride=2, dropout=par.conv_dropout[8])\n",
    "        # Comput the shape based on diff image size\n",
    "        __tmp = Variable(torch.zeros(1, 2, imsize1, imsize2))\n",
    "        __tmp = self.encode_image(__tmp)\n",
    "\n",
    "        # RNN\n",
    "        # self.rnn = nn.LSTM(\n",
    "        #             input_size=int(np.prod(__tmp.size())), \n",
    "        #             hidden_size=par.rnn_hidden_size, \n",
    "        #             num_layers=2, \n",
    "        #             dropout=par.rnn_dropout_between, \n",
    "        #             batch_first=True)\n",
    "        \n",
    "        self.rnn = Cfc(in_features=int(np.prod(__tmp.size())),\n",
    "                      hidden_size=par.rnn_hidden_size,\n",
    "                      out_feature=6,\n",
    "                      return_sequences=True,\n",
    "                      hparams=CFC)\n",
    "        \n",
    "        \n",
    "        # self.rnn_drop_out = nn.Dropout(par.rnn_dropout_out)\n",
    "        # self.linear = nn.Linear(in_features=par.rnn_hidden_size, out_features=6)\n",
    "\n",
    "        # Initilization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "                kaiming_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.LSTM):\n",
    "                # layer 1\n",
    "                kaiming_normal_(m.weight_ih_l0)  #orthogonal_(m.weight_ih_l0)\n",
    "                kaiming_normal_(m.weight_hh_l0)\n",
    "                m.bias_ih_l0.data.zero_()\n",
    "                m.bias_hh_l0.data.zero_()\n",
    "                # Set forget gate bias to 1 (remember)\n",
    "                n = m.bias_hh_l0.size(0)\n",
    "                start, end = n//4, n//2\n",
    "                m.bias_hh_l0.data[start:end].fill_(1.)\n",
    "\n",
    "                # layer 2\n",
    "                kaiming_normal_(m.weight_ih_l1)  #orthogonal_(m.weight_ih_l1)\n",
    "                kaiming_normal_(m.weight_hh_l1)\n",
    "                m.bias_ih_l1.data.zero_()\n",
    "                m.bias_hh_l1.data.zero_()\n",
    "                n = m.bias_hh_l1.size(0)\n",
    "                start, end = n//4, n//2\n",
    "                m.bias_hh_l1.data[start:end].fill_(1.)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x, times): \n",
    "        # x: (batch, seq_len, channel, width, height)\n",
    "        # stack_image\n",
    "        x = torch.cat(( x[:, :-1], x[:, 1:]), dim=2)\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        # CNN\n",
    "        x = x.view(batch_size*seq_len, x.size(2), x.size(3), x.size(4))\n",
    "        x = self.encode_image(x)\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "        \n",
    "#         print(\"Input shape to RNN: \", x.shape)\n",
    "        \n",
    "        t_elapsed = times[:, 1:] - times[:, :-1]\n",
    "        t_fill = torch.zeros(times.size(0), 1, device=x.device)\n",
    "        t = torch.cat((t_fill, t_elapsed), dim=1)\n",
    "\n",
    "        t = t * CFC[\"tau\"]\n",
    "\n",
    "        # RNN\n",
    "        out = self.rnn(x, t)\n",
    "        # out = self.rnn_drop_out(out)\n",
    "        # out = self.linear(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def encode_image(self, x):\n",
    "        out_conv2 = self.conv2(self.conv1(x))\n",
    "        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n",
    "        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n",
    "        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n",
    "        out_conv6 = self.conv6(out_conv5)\n",
    "        return out_conv6\n",
    "\n",
    "    def weight_parameters(self):\n",
    "        return [param for name, param in self.named_parameters() if 'weight' in name]\n",
    "\n",
    "    def bias_parameters(self):\n",
    "        return [param for name, param in self.named_parameters() if 'bias' in name]\n",
    "\n",
    "    def get_loss(self, x, times, y):\n",
    "        predicted = self.forward(x, times)\n",
    "        y = y[:, 1:, :]  # (batch, seq, dim_pose)\n",
    "        # Weighted MSE Loss\n",
    "#         print(\"Predicted shape: \", predicted.shape)\n",
    "#         print(\"Y shape: \", y.shape)\n",
    "        angle_loss = torch.nn.functional.mse_loss(predicted[:,:,:3], y[:,:,:3])\n",
    "        translation_loss = torch.nn.functional.mse_loss(predicted[:,:,3:], y[:,:,3:])\n",
    "        loss = (100 * angle_loss + translation_loss)\n",
    "        return loss\n",
    "\n",
    "    def step(self, x, times, y, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.get_loss(x, times, y)\n",
    "        loss.backward()\n",
    "        if self.clip != None:\n",
    "            torch.nn.utils.clip_grad_norm(self.rnn.parameters(), self.clip)\n",
    "        optimizer.step()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7178d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Dataloader_loss.py\n",
    "\n",
    "from params import par\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.modules import loss\n",
    "from torch import functional as F\n",
    "\n",
    "\n",
    "###############################################################\n",
    "#Transform Rotation Matrix to Euler Angle\n",
    "###############################################################\n",
    "# Checks if a matrix is a valid rotation matrix.\n",
    "def isRotationMatrix(R) :\n",
    "    Rt = np.transpose(R)\n",
    "    shouldBeIdentity = np.dot(Rt, R)\n",
    "    I = np.identity(3, dtype = R.dtype)\n",
    "    n = np.linalg.norm(I - shouldBeIdentity)\n",
    "    return n < 1e-6\n",
    " \n",
    " \n",
    "# Calculates rotation matrix to euler angles\n",
    "# The result is the same as MATLAB except the order\n",
    "# of the euler angles ( x and z are swapped ).\n",
    "def rotationMatrixToEulerAngles(R) :\n",
    " \n",
    "    assert(isRotationMatrix(R))\n",
    "     \n",
    "    sy = math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0])\n",
    "     \n",
    "    singular = sy < 1e-6\n",
    " \n",
    "    if  not singular :\n",
    "        x = math.atan2(R[2,1] , R[2,2])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = math.atan2(R[1,0], R[0,0])\n",
    "    else :\n",
    "        x = math.atan2(-R[1,2], R[1,1])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = 0\n",
    " \n",
    "    return np.array([x, y, z])\n",
    "\n",
    "###############################################################\n",
    "#DataLoader\n",
    "###############################################################\n",
    "\n",
    "#only fixed seq_len is used\n",
    "class KITTI_Data(Dataset):\n",
    "    def __init__(self,folder,seq_len): \n",
    "        \n",
    "        #only store images address in dataloader \n",
    "        root_train = 'dataset/dataset/sequences/{}/image_0'.format(folder)\n",
    "        imgs = os.listdir(root_train)\n",
    "        self.imgs = [os.path.join(root_train, img) for img in imgs]\n",
    "        self.imgs.sort()\n",
    "        self.GT = readGT('dataset/dataset/poses/{}.txt'.format(folder))\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #check index, CAUTION:the start/end of frames should be determined by GroundTruth file\n",
    "        try:\n",
    "        #Check the boundary, the lower boundary = frames-(seqs-1)-#StackNum\n",
    "            self.GT[index+self.seq_len]\n",
    "        except Exception:\n",
    "            print(\"Error:Index OutofRange\")\n",
    "        filenames = []\n",
    "        \n",
    "        #load path to images,read image and resemble as RGB\n",
    "        #NumofImgs = seqs + #StackNum\n",
    "        filenames = [self.imgs[index+i] for i in range(self.seq_len+1)]\n",
    "        images = [np.asarray(cv2.imread(img),dtype=np.float32) for img in filenames]\n",
    "\n",
    "        #resemble images as RGB\n",
    "        images = [img[:, :, (2, 1, 0)] for img in images]\n",
    "        \n",
    "        #Transpose the image that channels num. as first dimension\n",
    "        images = [np.transpose(img,(2,0,1)) for img in images]\n",
    "        images = [torch.from_numpy(img) for img in images]\n",
    "        \n",
    "        #stack per 2 images\n",
    "        images = [np.concatenate((images[k],images[k+1]),axis = 0) for k in range(len(images)-1)]\n",
    "                \n",
    "        \n",
    "        #prepare ground truth poses data\n",
    "\n",
    "        #Stack the images for seqs\n",
    "        return np.stack(images,axis = 0), self.GT[index:index+par.seq_len,:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.GT.shape[0]-1-par.seq_len-1\n",
    "\n",
    "#read groundtruth and return np.array\n",
    "def readGT(root):\n",
    "    with open(root, 'r') as posefile:\n",
    "        #read ground truth\n",
    "        GT = []\n",
    "        for one_line in posefile:\n",
    "            one_line = one_line.split(' ')\n",
    "            one_line = [float(pose) for pose in one_line]\n",
    "            #r = np.empty((3,3))\n",
    "            #r[:] = ([one_line[0:3],one_line[4:7],one_line[8:11]])\n",
    "            gt = np.append(rotationMatrixToEulerAngles(np.matrix([one_line[0:3], one_line[4:7], one_line[8:11]])), np.array([one_line[3], one_line[7], one_line[11]]))\n",
    "            GT.append(gt)\n",
    "    return np.array(GT, dtype=np.float32)\n",
    "        \n",
    "###############################################################\n",
    "#Custom Loss Function\n",
    "###############################################################\n",
    "\n",
    "class DeepvoLoss(loss._Loss):\n",
    "    def __init__(self, size_average=True, reduce=True):\n",
    "        super(DeepvoLoss, self).__init__()    \n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return F.mse_loss(input[0:3], target[0:3], size_average=self.size_average, reduce=self.reduce)+100 * F.mse_loss(input[3:6], target[3:6], size_average=self.size_average, reduce=self.reduce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae19f523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning 00 directory\n",
      "Cleaning 01 directory\n",
      "Cleaning 02 directory\n",
      "Cleaning 03 directory\n",
      "Cleaning 04 directory\n",
      "Cleaning 05 directory\n",
      "Cleaning 06 directory\n",
      "Cleaning 07 directory\n",
      "Cleaning 08 directory\n",
      "Cleaning 09 directory\n",
      "Cleaning 10 directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "from params import par\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import math\n",
    "from helper import R_to_angle\n",
    "\n",
    "def clean_unused_images():\n",
    "    seq_frame = {'00': ['000', '004540'],\n",
    "                '01': ['000', '001100'],\n",
    "                '02': ['000', '004660'],\n",
    "                '03': ['000', '000800'],\n",
    "                '04': ['000', '000270'],\n",
    "                '05': ['000', '002760'],\n",
    "                '06': ['000', '001100'],\n",
    "                '07': ['000', '001100'],\n",
    "                '08': ['001100', '005170'],\n",
    "                '09': ['000', '001590'],\n",
    "                '10': ['000', '001200']\n",
    "                }\n",
    "    for dir_id, img_ids in seq_frame.items():\n",
    "        dir_path = '{}{}/'.format(par.image_dir, dir_id)\n",
    "        if not os.path.exists(dir_path):\n",
    "            continue\n",
    "\n",
    "        print('Cleaning {} directory'.format(dir_id))\n",
    "        start, end = img_ids\n",
    "        start, end = int(start), int(end)\n",
    "        for idx in range(0, start):\n",
    "            img_name = '{:010d}.png'.format(idx)\n",
    "            img_path = '{}{}/{}'.format(par.image_dir, dir_id, img_name)\n",
    "            if os.path.isfile(img_path):\n",
    "                os.remove(img_path)\n",
    "        for idx in range(end+1, 10000):\n",
    "            img_name = '{:010d}.png'.format(idx)\n",
    "            img_path = '{}{}/{}'.format(par.image_dir, dir_id, img_name)\n",
    "            if os.path.isfile(img_path):\n",
    "                os.remove(img_path)\n",
    "                \n",
    "        clean_times = []\n",
    "        original_times = []\n",
    "        with open(f'{dir_path}/times.txt', 'r') as f:\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                if i >= start and i <= end:\n",
    "                    clean_times.append(line)\n",
    "        with open(f'{dir_path}/clean_times.txt', 'w') as f:\n",
    "            for line in clean_times:\n",
    "                f.write(line)\n",
    "            \n",
    "\n",
    "\n",
    "# transform poseGT [R|t] to [theta_x, theta_y, theta_z, x, y, z]\n",
    "# save as .npy file\n",
    "def create_pose_data():\n",
    "    info = {'00': [0, 4540], '01': [0, 1100], '02': [0, 4660], '03': [0, 800], '04': [0, 270], '05': [0, 2760], '06': [0, 1100], '07': [0, 1100], '08': [1100, 5170], '09': [0, 1590], '10': [0, 1200]}\n",
    "    start_t = time.time()\n",
    "    for video in info.keys():\n",
    "        fn = '{}{}.txt'.format(par.pose_dir, video)\n",
    "        print('Transforming {}...'.format(fn))\n",
    "        with open(fn) as f:\n",
    "            lines = [line.split('\\n')[0] for line in f.readlines()]\n",
    "            poses = [R_to_angle([float(value) for value in l.split(' ')]) for l in lines]  # list of pose (pose=list of 12 floats)\n",
    "            poses = np.array(poses)\n",
    "            base_fn = os.path.splitext(fn)[0]\n",
    "            np.save(base_fn+'.npy', poses)\n",
    "            print('Video {}: shape={}'.format(video, poses.shape))\n",
    "    print('elapsed time = {}'.format(time.time()-start_t))\n",
    "\n",
    "\n",
    "def calculate_bw_mean_std(image_path_list, minus_point_5=False):\n",
    "    n_images = len(image_path_list)\n",
    "    cnt_pixels = 0\n",
    "    print('Numbers of frames in training dataset: {}'.format(n_images))\n",
    "    mean_np = 0.0\n",
    "    mean_tensor = 0.0\n",
    "    to_tensor = transforms.ToTensor()\n",
    "\n",
    "    image_sequence = []\n",
    "    for idx, img_path in enumerate(image_path_list):\n",
    "        print('{} / {}'.format(idx, n_images), end='\\r')\n",
    "        img_as_img = Image.open(img_path)\n",
    "        img_as_tensor = to_tensor(img_as_img)\n",
    "        if minus_point_5:\n",
    "            img_as_tensor = img_as_tensor - 0.5\n",
    "        img_as_np = np.array(img_as_img)\n",
    "        # img_as_np = np.rollaxis(img_as_np, 2, 0)\n",
    "        cnt_pixels += img_as_np.shape[0]*img_as_np.shape[1]\n",
    "\n",
    "        mean_tensor += float(torch.sum(img_as_tensor))\n",
    "        mean_np += float(np.sum(img_as_np))\n",
    "\n",
    "    mean_tensor = mean_tensor / cnt_pixels\n",
    "    mean_np = mean_np / cnt_pixels\n",
    "    print('mean_tensor = ', mean_tensor)\n",
    "    print('mean_np = ', mean_np)\n",
    "\n",
    "    std_tensor = 0.0\n",
    "    std_np = 0.0\n",
    "    for idx, img_path in enumerate(image_path_list):\n",
    "        print('{} / {}'.format(idx, n_images), end='\\r')\n",
    "        img_as_img = Image.open(img_path)\n",
    "        img_as_tensor = to_tensor(img_as_img)\n",
    "        if minus_point_5:\n",
    "            img_as_tensor = img_as_tensor - 0.5\n",
    "        img_as_np = np.array(img_as_img)\n",
    "        # img_as_np = np.rollaxis(img_as_np, 2, 0)\n",
    "\n",
    "        tmp = (img_as_tensor - mean_tensor)**2\n",
    "        std_tensor += float(torch.sum(tmp))\n",
    "        tmp = (img_as_np - mean_np)**2\n",
    "        std_np += float(np.sum(tmp))\n",
    "\n",
    "    std_tensor = math.sqrt(std_tensor / cnt_pixels)\n",
    "    std_np = math.sqrt(std_np / cnt_pixels)\n",
    "    print('std_tensor = ', std_tensor)\n",
    "    print('std_np = ', std_np)\n",
    "\n",
    "\n",
    "clean_unused_images()\n",
    "run = False\n",
    "if run:\n",
    "    create_pose_data()\n",
    "\n",
    "    # Calculate Black & White means of images in training videos\n",
    "    train_video = ['00', '02', '08', '09', '06', '04', '10']\n",
    "    image_path_list = []\n",
    "    for folder in train_video:\n",
    "        image_path_list += glob.glob('dataset/dataset/sequences/{}/image_0/*.png'.format(folder))\n",
    "    calculate_bw_mean_std(image_path_list, minus_point_5=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data info from datainfo/train_df_t000102050809_v04060710_pNone_seq5x7_sample1.pickle\n",
      "Number of samples in training dataset:  3735\n",
      "Number of samples in validation dataset:  739\n",
      "==================================================\n",
      "CUDA used.\n",
      "Record loss in:  records/t000102050809_v04060710_im184x608_s5x7_b8_rnn448_optAdagrad_lr0.0005.txt\n",
      "==================================================\n",
      "Train take 441.8 sec.79% - Loss = 1.3996717929840088\n",
      "Valid take 31.1 sec 99.79%\n",
      "Epoch 1\n",
      "train loss mean: 4.618072225044726, std: 22.94\n",
      "valid loss mean: 0.289768988778303, std: 0.06\n",
      "\n",
      "Save model at ep 1, mean of valid loss: 0.289768988778303\n",
      "Save model at ep 1, mean of train loss: 4.618072225044726\n",
      "==================================================\n",
      "Train take 442.9 sec.79% - Loss = 0.8260985016822815\n",
      "Valid take 33.5 sec 99.79%\n",
      "Epoch 2\n",
      "train loss mean: 1.1337435984816162, std: 0.22\n",
      "valid loss mean: 0.16532632348301646, std: 0.03\n",
      "\n",
      "Save model at ep 2, mean of valid loss: 0.16532632348301646\n",
      "Save model at ep 2, mean of train loss: 1.1337435984816162\n",
      "==================================================\n",
      "Train take 461.5 sec.79% - Loss = 0.69068264961242683\n",
      "Valid take 31.3 sec 99.79%\n",
      "Epoch 3\n",
      "train loss mean: 0.8095165786569211, std: 0.15\n",
      "valid loss mean: 0.1386220823604982, std: 0.03\n",
      "\n",
      "Save model at ep 3, mean of valid loss: 0.1386220823604982\n",
      "Save model at ep 3, mean of train loss: 0.8095165786569211\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from params import par\n",
    "from data_helper import get_data_info, SortedRandomBatchSampler, ImageSequenceDataset, get_partition_data_info\n",
    "\n",
    "\n",
    "# Write all hyperparameters to record_path\n",
    "mode = 'a' if par.resume else 'w'\n",
    "with open(par.record_path, mode) as f:\n",
    "    f.write('\\n'+'='*50 + '\\n')\n",
    "    f.write('\\n'.join(\"%s: %s\" % item for item in vars(par).items()))\n",
    "    f.write('\\n'+'='*50 + '\\n')\n",
    "\n",
    "# Prepare Data\n",
    "if os.path.isfile(par.train_data_info_path) and os.path.isfile(par.valid_data_info_path):\n",
    "    print('Load data info from {}'.format(par.train_data_info_path))\n",
    "    train_df = pd.read_pickle(par.train_data_info_path)\n",
    "    valid_df = pd.read_pickle(par.valid_data_info_path)\n",
    "else:\n",
    "    print('Create new data info')\n",
    "    if par.partition != None:\n",
    "        partition = par.partition\n",
    "        train_df, valid_df = get_partition_data_info(partition, par.train_video, par.seq_len, overlap=1, sample_times=par.sample_times, shuffle=True, sort=True)\n",
    "    else:\n",
    "        train_df = get_data_info(folder_list=par.train_video, seq_len_range=par.seq_len, overlap=1, sample_times=par.sample_times)\n",
    "        valid_df = get_data_info(folder_list=par.valid_video, seq_len_range=par.seq_len, overlap=1, sample_times=par.sample_times)\n",
    "    # save the data info\n",
    "    train_df.to_pickle(par.train_data_info_path)\n",
    "    valid_df.to_pickle(par.valid_data_info_path)\n",
    "\n",
    "train_sampler = SortedRandomBatchSampler(train_df, par.batch_size, drop_last=True)\n",
    "train_dataset = ImageSequenceDataset(train_df, par.resize_mode, (par.img_w, par.img_h), par.img_means, par.img_stds, par.minus_point_5)\n",
    "train_dl = DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=par.n_processors, pin_memory=par.pin_mem)\n",
    "\n",
    "valid_sampler = SortedRandomBatchSampler(valid_df, par.batch_size, drop_last=True)\n",
    "valid_dataset = ImageSequenceDataset(valid_df, par.resize_mode, (par.img_w, par.img_h), par.img_means, par.img_stds, par.minus_point_5)\n",
    "valid_dl = DataLoader(valid_dataset, batch_sampler=valid_sampler, num_workers=par.n_processors, pin_memory=par.pin_mem)\n",
    "\n",
    "print('Number of samples in training dataset: ', len(train_df.index))\n",
    "print('Number of samples in validation dataset: ', len(valid_df.index))\n",
    "print('='*50)\n",
    "\n",
    "\n",
    "# Model\n",
    "M_deepvo = DeepVO(par.img_h, par.img_w, par.batch_norm)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('CUDA used.')\n",
    "    M_deepvo = M_deepvo.cuda()\n",
    "\n",
    "\n",
    "# Load FlowNet weights pretrained with FlyingChairs\n",
    "# NOTE: the pretrained model assumes image rgb values in range [-0.5, 0.5]\n",
    "if par.pretrained_flownet and not par.resume:\n",
    "    if use_cuda:\n",
    "        pretrained_w = torch.load(par.pretrained_flownet)\n",
    "    else:\n",
    "        pretrained_w = torch.load(par.pretrained_flownet_flownet, map_location='cpu')\n",
    "    print('Load FlowNet pretrained model')\n",
    "    # Use only conv-layer-part of FlowNet as CNN for DeepVO\n",
    "    model_dict = M_deepvo.state_dict()\n",
    "    update_dict = {k: v for k, v in pretrained_w['state_dict'].items() if k in model_dict}\n",
    "    model_dict.update(update_dict)\n",
    "    M_deepvo.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "# Create optimizer\n",
    "if par.optim['opt'] == 'Adam':\n",
    "    optimizer = torch.optim.Adam(M_deepvo.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "elif par.optim['opt'] == 'Adagrad':\n",
    "    optimizer = torch.optim.Adagrad(M_deepvo.parameters(), lr=par.optim['lr'])\n",
    "elif par.optim['opt'] == 'Cosine':\n",
    "    optimizer = torch.optim.SGD(M_deepvo.parameters(), lr=par.optim['lr'])\n",
    "    T_iter = par.optim['T']*len(train_dl)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_iter, eta_min=0, last_epoch=-1)\n",
    "\n",
    "# Load trained DeepVO model and optimizer\n",
    "if par.resume:\n",
    "    M_deepvo.load_state_dict(torch.load(par.load_model_path))\n",
    "    optimizer.load_state_dict(torch.load(par.load_optimizer_path))\n",
    "    print('Load model from: ', par.load_model_path)\n",
    "    print('Load optimizer from: ', par.load_optimizer_path)\n",
    "\n",
    "\n",
    "# Train\n",
    "print('Record loss in: ', par.record_path)\n",
    "min_loss_t = 1e10\n",
    "min_loss_v = 1e10\n",
    "M_deepvo.train()\n",
    "for ep in range(par.epochs):\n",
    "    st_t = time.time()\n",
    "    print('='*50)\n",
    "    # Train\n",
    "    M_deepvo.train()\n",
    "    loss_mean = 0\n",
    "    t_loss_list = []\n",
    "    epoch_length = len(train_dl)\n",
    "    for i, (_, t_x, t_times, t_y) in enumerate(train_dl):\n",
    "        if use_cuda:\n",
    "            t_x = t_x.cuda(non_blocking=par.pin_mem)\n",
    "            t_y = t_y.cuda(non_blocking=par.pin_mem)\n",
    "            t_times = t_times.cuda(non_blocking=par.pin_mem)\n",
    "        ls = M_deepvo.step(t_x, t_times, t_y, optimizer).data.cpu().numpy()\n",
    "        t_loss_list.append(float(ls))\n",
    "        loss_mean += float(ls)\n",
    "        if par.optim == 'Cosine':\n",
    "            lr_scheduler.step()\n",
    "        print(f'Training epoch {ep}: {(100.0*i/epoch_length):.2f}% - Loss = {float(ls)}', end='\\r')\n",
    "    print('\\nTrain take {:.1f} sec'.format(time.time()-st_t))\n",
    "    loss_mean /= len(train_dl)\n",
    "\n",
    "    # Validation\n",
    "    st_t = time.time()\n",
    "    M_deepvo.eval()\n",
    "    loss_mean_valid = 0\n",
    "    v_loss_list = []\n",
    "    for _, v_x, v_times, v_y in valid_dl:\n",
    "        print(f'Validating epoch {ep}: {(100.0*i/epoch_length):.2f}%', end='\\r')\n",
    "        if use_cuda:\n",
    "            v_x = v_x.cuda(non_blocking=par.pin_mem)\n",
    "            v_y = v_y.cuda(non_blocking=par.pin_mem)\n",
    "            v_times = v_times.cuda(non_blocking=par.pin_mem)\n",
    "            v_ls = M_deepvo.get_loss(v_x, v_times, v_y).data.cpu().numpy()\n",
    "        v_loss_list.append(float(v_ls))\n",
    "        loss_mean_valid += float(v_ls)\n",
    "    print('\\nValid take {:.1f} sec'.format(time.time()-st_t))\n",
    "    loss_mean_valid /= len(valid_dl)\n",
    "\n",
    "\n",
    "    f = open(par.record_path, 'a')\n",
    "    f.write('Epoch {}\\ntrain loss mean: {}, std: {:.2f}\\nvalid loss mean: {}, std: {:.2f}\\n'.format(ep+1, loss_mean, np.std(t_loss_list), loss_mean_valid, np.std(v_loss_list)))\n",
    "    print('Epoch {}\\ntrain loss mean: {}, std: {:.2f}\\nvalid loss mean: {}, std: {:.2f}\\n'.format(ep+1, loss_mean, np.std(t_loss_list), loss_mean_valid, np.std(v_loss_list)))\n",
    "\n",
    "    # Save model\n",
    "    # save if the valid loss decrease\n",
    "    check_interval = 1\n",
    "    if loss_mean_valid < min_loss_v and ep % check_interval == 0:\n",
    "        min_loss_v = loss_mean_valid\n",
    "        print('Save model at ep {}, mean of valid loss: {}'.format(ep+1, loss_mean_valid))  # use 4.6 sec \n",
    "        torch.save(M_deepvo.state_dict(), par.save_model_path+'.valid')\n",
    "        torch.save(optimizer.state_dict(), par.save_optimzer_path+'.valid')\n",
    "    # save if the training loss decrease\n",
    "    check_interval = 1\n",
    "    if loss_mean < min_loss_t and ep % check_interval == 0:\n",
    "        min_loss_t = loss_mean\n",
    "        print('Save model at ep {}, mean of train loss: {}'.format(ep+1, loss_mean))\n",
    "        torch.save(M_deepvo.state_dict(), par.save_model_path+'.train')\n",
    "        torch.save(optimizer.state_dict(), par.save_optimzer_path+'.train')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b7cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
